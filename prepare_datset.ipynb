{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('s200862/medical_qa_meds', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sample(sample):\n",
    "    # Extract the prompt\n",
    "    prompt_start = sample.find('[INST]') + len('[INST]')\n",
    "    prompt_end = sample.find('[/INST]')\n",
    "    prompt = sample[prompt_start:prompt_end].strip()\n",
    "    \n",
    "    # Extract the symptoms\n",
    "    symptoms = sample[prompt_end + len('[/INST]'):].strip()\n",
    "    \n",
    "    return prompt, symptoms\n",
    "\n",
    "# Initialize lists to store parsed data\n",
    "prompts = []\n",
    "symptoms_list = []\n",
    "\n",
    "# Parse each sample\n",
    "\n",
    "parsed_samples = list(map(parse_sample, train_dataset['text']))\n",
    "prompts, symptoms_list = zip(*parsed_samples)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'Prompt': prompts,\n",
    "    'Symptoms': symptoms_list\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to split dataset into train, validation, and test sets\n",
    "def split_dataset(dataset, train_size=0.8, val_size=0.1, test_size=0.1):\n",
    "    assert train_size + val_size + test_size == 1, \"Train, val, and test sizes must sum to 1.\"\n",
    "    \n",
    "    train_test_split = dataset.train_test_split(test_size=(val_size + test_size))\n",
    "    train_set = train_test_split['train']\n",
    "    \n",
    "    val_test_split = train_test_split['test'].train_test_split(test_size=test_size / (val_size + test_size))\n",
    "    val_set = val_test_split['train']\n",
    "    test_set = val_test_split['test']\n",
    "    \n",
    "    return DatasetDict({\n",
    "        'train': train_set,\n",
    "        'validation': val_set,\n",
    "        'test': test_set\n",
    "    })\n",
    "\n",
    "# Split the dataset\n",
    "dataset_dict = split_dataset(dataset, train_size=0.8, val_size=0.1, test_size=0.1)\n",
    "\n",
    "# Verify the splits\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict.push_to_hub(\"medical_datset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Choose max length for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['Prompt']}\\n ### Answer: {example['Symptoms']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Prajapat/medical_datset',split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    # lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
